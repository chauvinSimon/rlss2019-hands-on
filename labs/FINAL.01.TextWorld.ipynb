{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5w08Bz0m2Ra"
   },
   "source": [
    "# TextWorld 101\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kT9ZurVxn3oa"
   },
   "source": [
    "## Environment setup\n",
    "\n",
    "* TextWorld (I used my branch instead of microsoft upstream version [here](https://github.com/Microsoft/TextWorld) to freeze TextWorld, as it is being actively developed)\n",
    "* pytorch\n",
    "* ptan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2m-ULnq5DxLO"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/Shmuma/TextWorld > /dev/null 2>&1\n",
    "!pip install ptan > /dev/null 2>&1\n",
    "!pip3 install torch torchvision > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "colab_type": "code",
    "id": "5Tsi-vvKD9hu",
    "outputId": "bec33154-c2b8-40bf-f5e2-827ae7891ec3"
   },
   "outputs": [],
   "source": [
    "# Juggling around colab requirement\n",
    "!pip uninstall -y prompt-toolkit > /dev/null 2>&1\n",
    "!pip install prompt-toolkit==1.0.16\n",
    "# DO NOT FORGET to restart runtime as suggested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTWnBRj_tpd3"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1YDwj_PTtxP3"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import textworld.gym as tw_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLvzoUN8VLRM"
   },
   "source": [
    "## Build games we'll play with\n",
    "\n",
    "TextWorld allows to generate games of specific complexity, which makes it great for experimentation, learning and reasearch, as we can gradually increase complexity of problems.\n",
    "\n",
    "Below we generate the problem with 10 objects, 5 rooms and quest length 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "iw_vwZ_gVe9J",
    "outputId": "ccdd360b-4e19-41dd-cf78-01b9e6c5b45a"
   },
   "outputs": [],
   "source": [
    "!tw-make custom --world-size 5 --nb-objects 10 --quest-length 5 --seed 1234 --output simple1.ulx\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYkXHfF6o957"
   },
   "source": [
    "\n",
    "## Env capabilities\n",
    "\n",
    "Explore textworld gym environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7TKWGF-ppAcY"
   },
   "outputs": [],
   "source": [
    "env_id = tw_gym.register_game(\"simple1.ulx\")\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ig7o6cMPEGqg",
    "outputId": "cf57f2a3-7503-46d2-a52f-55f29b419e74"
   },
   "outputs": [],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4GCyCNXdEhTB",
    "outputId": "0dc68448-d857-4448-9661-567836058d1f"
   },
   "outputs": [],
   "source": [
    "env.observation_space.vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "yqS77YN1ZTeM",
    "outputId": "316ebb44-81b9-4602-840f-500aac7c2a6e"
   },
   "outputs": [],
   "source": [
    "r = env.reset()\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "id": "BzWIsqg5ZYd6",
    "outputId": "147cc42f-73d2-4266-dfd2-8c8b275aeb27"
   },
   "outputs": [],
   "source": [
    "print(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3idrRH3PZml_",
    "outputId": "84dca56c-03e6-4f5c-8d43-b19ffb55bd88"
   },
   "outputs": [],
   "source": [
    "r[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Abpy20zpZrKA"
   },
   "source": [
    "So, TextWorld env is **incompatible** with gym api, as `reset()` returns tuple with observation string and extended info dict. This is not a bug, but a feature -- gym provides no way to get the extended info for initial observation.\n",
    "\n",
    "We need to take this into account on observation preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q2jxVnVxayyt",
    "outputId": "31bf6f6a-e85f-46e3-9ea5-422af68d78a7"
   },
   "outputs": [],
   "source": [
    "r1 = env.step('go east')\n",
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "OolFbdCta6JE",
    "outputId": "c1baf3a9-278b-4a7b-a275-1e14a989fb68"
   },
   "outputs": [],
   "source": [
    "print(r1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGZzRkM4bDKa"
   },
   "source": [
    "## Extended attributes\n",
    "\n",
    "TextWorld can provide us information in a more structured way, giving information about admissible commands, intermediate reward, inventory and even path to solution.\n",
    "\n",
    "To get it, we need to pass specific flags on environment creation.\n",
    "\n",
    "Below we'll ask about specific flags I find useful, full set of them is here: https://github.com/microsoft/TextWorld/blob/04952aa64c4612309a7296aef99867ecfa5c7db1/textworld/envs/wrappers/filter.py\n",
    "\n",
    "In addition, we can limit amount of steps agent can do and redefine the name of gym env which will be registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nPPknG5ObE93",
    "outputId": "ce1cd0e5-58c8-4903-9024-9a1bf9fe5400"
   },
   "outputs": [],
   "source": [
    "from textworld.envs.wrappers.filter import EnvInfos\n",
    "\n",
    "EXTRA_GAME_INFO = {\n",
    "    \"inventory\": True,\n",
    "    \"description\": True,\n",
    "    \"intermediate_reward\": True,\n",
    "    \"admissible_commands\": True,\n",
    "    \"policy_commands\": True,\n",
    "}\n",
    "\n",
    "env_id = tw_gym.register_game(\"simple1.ulx\", max_episode_steps=10,\n",
    "                                  name=\"simple1\", request_infos=EnvInfos(**EXTRA_GAME_INFO))\n",
    "env_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dwnG3haCdJnK"
   },
   "outputs": [],
   "source": [
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "v2sIRq7WdNlL",
    "outputId": "fc1fdece-9016-43fe-c3a3-40a30cc92468"
   },
   "outputs": [],
   "source": [
    "r = env.reset()\n",
    "r[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZrRB2_-F8kK"
   },
   "source": [
    "The dict returned by env contains the following fields:\n",
    "\n",
    "\n",
    "*   `admissible_commands`: list of commands (=actions) we can execute from the current state\n",
    "*   `description`: generic description of the scene (observation)\n",
    "* `intermediate_reward`: additional reward showing us we're on the right or wrong track upon solution. TextWorld description says it equals +1 every time path to the solution becomes shorter and -1 if we do something which pushes us away from final game goal\n",
    "* `inventory`: description of things you're carrying\n",
    "* `policy_commands`: list of commands we need to execute to get to the goal from the current state. We're not going to use this information during the training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G69OTcDLfXLW"
   },
   "source": [
    "## Observations and actions\n",
    "\n",
    "Both observation and action space are of new `Word` class, which is an extension of TextWorld, representing variable-length sequence of words from some vocabulary.\n",
    "\n",
    "Code is here: https://github.com/microsoft/TextWorld/blob/40e13d9bd1d3bb64a98504a8d50d8873884cc08c/textworld/gym/spaces/text_spaces.py#L86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "E3JCA48UfUG9",
    "outputId": "5c69b9c3-e2a7-4f5d-869f-0974dcc98665"
   },
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4NUnm7QsgZsS",
    "outputId": "b44ef205-8a76-41ce-9410-15f26f3eb626"
   },
   "outputs": [],
   "source": [
    "tokens = env.observation_space.tokenize(\"You find yourself in a scullery\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4mLXl0UDgwaB",
    "outputId": "5ec1ccc5-d72e-4893-d2a9-1af64c77720a"
   },
   "outputs": [],
   "source": [
    "env.observation_space.vocab[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SAA2c3bWhBlI",
    "outputId": "019111eb-e9b9-4338-a907-b9dc598cbb94"
   },
   "outputs": [],
   "source": [
    "[env.observation_space.vocab[t] for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mydhPrcWhaDQ"
   },
   "source": [
    "# Baseline DQN agent\n",
    "\n",
    "First step will be the DQN which takes the state description and one of the commands from 'admissible commands' list and estimate Q-value using the Bellman equation.\n",
    "\n",
    "---\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/Shmuma/rlss2019-hands-on/textworld/imgs/RLSS-TW-arch.png)\n",
    "\n",
    "## Preprocessor pipeline\n",
    "\n",
    "Before we can start training the agent, we need to convert the data into NN-friendly form. DQN net accepts the fixed-sized vector which encodes state description and command we're going to execute.\n",
    "\n",
    "Preprocessing consists of several steps:\n",
    "* from raw strings given by environment to sequence of token ids -- indices in underlying vocabulary. This step is already implemented by TextWorld in `Word.tokenize()` method, but we need to implement its call in a convenient and flexible way.\n",
    "* every token in sequence should be encoded into dense vector using so called *Embeddings*, which is a standard NLP practice to get from individual sequence entries into NN representation. You can take pre-trained embeddings or train your own using word2vec or glove methods. In our demo we go with third way: train embeddings as part of our model.\n",
    "* Embeddings convert every token in sequence into dense vector of fixed size, but the sequence as a whole still has variable length. To deal with that variability we'll use RNNs, which, being applied to input sequence can encode it into fixed size vector, capturing the squence as a whole. This step is labelled as Encoders on the diagram above. We'll train invidiual encoder for every input sequence.\n",
    "* Every encoder outputs fixed-size vector capturing the specifics from the sequence, so we just concatenate them together to get single fixed-size input to DQN network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOjCmk0lMZrL"
   },
   "outputs": [],
   "source": [
    "import ptan\n",
    "import random\n",
    "import pathlib\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Iterable, Optional, Any\n",
    "from textworld.gym import spaces as tw_spaces\n",
    "from textworld.envs.wrappers.filter import EnvInfos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_JZHhbL-3gL"
   },
   "source": [
    "## Env encoder wrapper\n",
    "\n",
    "To simplify further experiments, below is the wrapper around TextWorld environment which transforms the observations using `Word.tokenize()` method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kcAZQQk8CYjd"
   },
   "outputs": [],
   "source": [
    "STEPS_LIMIT = 50\n",
    "GAME_FILE = \"simple1.ulx\"\n",
    "\n",
    "EXTRA_GAME_INFO = {\n",
    "  \"inventory\": True,\n",
    "  \"description\": True,\n",
    "  \"intermediate_reward\": True,\n",
    "  \"admissible_commands\": True,\n",
    "}\n",
    "\n",
    "\n",
    "def make_env():\n",
    "  game_path = pathlib.Path(GAME_FILE)\n",
    "  env_id = tw_gym.register_game(\n",
    "      str(game_path), max_episode_steps=STEPS_LIMIT,\n",
    "      name=game_path.stem, request_infos=EnvInfos(**EXTRA_GAME_INFO))\n",
    "  env = gym.make(env_id)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BmoNnM_BBemy",
    "outputId": "55118855-0e1d-4da3-a763-9a7486c84248"
   },
   "outputs": [],
   "source": [
    "env = make_env()\n",
    "env\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dPe6Yp4__dBP"
   },
   "outputs": [],
   "source": [
    "class TextWorldPreproc(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Simple wrapper to preprocess text_world game observation\n",
    "    \"\"\"\n",
    "    log = logging.getLogger(\"TextWorldPreproc\")\n",
    "    \n",
    "    def __init__(self, env: gym.Env, encode_raw_text: bool = False,\n",
    "                 encode_extra_fields: Iterable[str] = ('description', 'inventory'),\n",
    "                 use_admissible_commands: bool = True,\n",
    "                 use_intermediate_reward: bool = True,\n",
    "                 tokens_limit: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        :param env: env to be wrapped. Has to provide Word observations\n",
    "        :param encode_raw_text: do we need to encode raw observation from environment, if true, adds an extra encoder\n",
    "        :param encode_extra_fields: tuple of field names to be encoded, expected to be string values\n",
    "        :param use_admissible_commands: if true, admissible commands used for action wrapping\n",
    "        :param use_intermediate_reward: take intermediate reward into account\n",
    "        :param tokens_limit: optional limit of tokens in the encoded fields\n",
    "        \"\"\"\n",
    "        super(TextWorldPreproc, self).__init__(env)\n",
    "        if not isinstance(env.observation_space, tw_spaces.Word):\n",
    "            raise ValueError(\"Env should expose text_world compatible observation space, \"\n",
    "                             \"this one gives %s\" % env.observation_space)\n",
    "        self._encode_raw_text = encode_raw_text\n",
    "        self._encode_extra_field = tuple(encode_extra_fields)\n",
    "        self._use_admissible_commands = use_admissible_commands\n",
    "        self._use_intermedate_reward = use_intermediate_reward\n",
    "        self._num_fields = len(self._encode_extra_field) + int(self._encode_raw_text)\n",
    "        self._last_admissible_commands = None\n",
    "        self._last_extra_info = None\n",
    "        self._tokens_limit = tokens_limit\n",
    "        self._cmd_hist = []\n",
    "\n",
    "    @property\n",
    "    def num_fields(self):\n",
    "        return self._num_fields\n",
    "\n",
    "    def _encode(self, obs: str, extra_info: dict) -> dict:\n",
    "        obs_result = []\n",
    "        if self._encode_raw_text:\n",
    "            tokens = self.env.observation_space.tokenize(obs)\n",
    "            if self._tokens_limit is not None:\n",
    "                tokens = tokens[:self._tokens_limit]\n",
    "            obs_result.append(tokens)\n",
    "        for field in self._encode_extra_field:\n",
    "            tokens = self.env.observation_space.tokenize(extra_info[field])\n",
    "            if self._tokens_limit is not None:\n",
    "                tokens = tokens[:self._tokens_limit]\n",
    "            obs_result.append(tokens)\n",
    "        result = {\"obs\": obs_result}\n",
    "        if self._use_admissible_commands:\n",
    "            adm_result = []\n",
    "            for cmd in extra_info['admissible_commands']:\n",
    "                adm_result.append(self.env.action_space.tokenize(cmd))\n",
    "            result['admissible_commands'] = adm_result\n",
    "            self._last_admissible_commands = extra_info['admissible_commands']\n",
    "        self._last_extra_info = extra_info\n",
    "        return result\n",
    "\n",
    "    # TextWorld environment has a workaround of gym drawback: \n",
    "    # reset returns tuple with raw observation and extra dict\n",
    "    def reset(self):\n",
    "        res = self.env.reset()\n",
    "        self._cmd_hist = []\n",
    "        return self._encode(res[0], res[1])\n",
    "\n",
    "    def step(self, action):\n",
    "        if self._use_admissible_commands:\n",
    "            action = self._last_admissible_commands[action]\n",
    "            self._cmd_hist.append(action)\n",
    "        obs, r, is_done, extra = self.env.step(action)\n",
    "        if self._use_intermedate_reward:\n",
    "            r += extra.get('intermediate_reward', 0)\n",
    "        new_extra = dict(extra)\n",
    "        for f in self._encode_extra_field + ('admissible_commands', 'intermediate_reward'):\n",
    "            if f in new_extra:\n",
    "                new_extra.pop(f)\n",
    "        # if is_done:\n",
    "        #     self.log.info(\"Commands: %s\", self._cmd_hist)\n",
    "        #     self.log.info(\"Reward: %s, extra: %s\", r, new_extra)\n",
    "        return self._encode(obs, extra), r, is_done, new_extra\n",
    "\n",
    "    @property\n",
    "    def last_admissible_commands(self):\n",
    "        return tuple(self._last_admissible_commands) if self._last_admissible_commands else None\n",
    "\n",
    "    @property\n",
    "    def last_extra_info(self):\n",
    "        return self._last_extra_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MXY1rvQDDqKx",
    "outputId": "711539c5-7e4f-4e80-bdc8-72dc0f3c8571"
   },
   "outputs": [],
   "source": [
    "e = TextWorldPreproc(env)\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cbaYMcpwI8JM"
   },
   "source": [
    "Preprocessor applies tokenisation to original environment observations and repacks it into dict with two fileds: `admissible_commands`, which is a list of commands to take and `obs` list, keeping tokenised sequences building up our observation. \n",
    "\n",
    "Let's check how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "ZyRzLN3CDun-",
    "outputId": "bf798fab-5377-44a7-f331-2793693fa468"
   },
   "outputs": [],
   "source": [
    "r = e.reset()\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7PmXI6oeJXKv"
   },
   "source": [
    "Besides transformed observations, the wrapper keeps two fields: `last_extra_info` with raw untransformed state from TextWorld environment and `last_admissible_commands` with list of text commands available. They are useful for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "nXdKj8rbECJt",
    "outputId": "df5392aa-6ede-4e9b-9547-9fc81d093490"
   },
   "outputs": [],
   "source": [
    "e.last_extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "VbIREqE5ELyj",
    "outputId": "de35b190-cb31-47a9-f8b6-2c4c8865e062"
   },
   "outputs": [],
   "source": [
    "e.step(e.last_admissible_commands.index('unlock gate with keycard'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfxgzqxuXrCJ"
   },
   "source": [
    "Please note reward=1 (second entry in returned tuple) in the result above. That's due to `intermediate_reward` option set in the env, which gives us reward every time we do a step in the right direction. You can disable this to make problem harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "5NjiXKxWIq_t",
    "outputId": "21694d65-fe3d-4c7d-83ed-0641b6341d19"
   },
   "outputs": [],
   "source": [
    "e.last_extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "ygHkW18sIvML",
    "outputId": "5b0c569c-1fcc-4aaa-9aab-917084a7f703"
   },
   "outputs": [],
   "source": [
    "e.step(e.last_admissible_commands.index('open gate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "Wbu2jyaYJCbj",
    "outputId": "b05b1ed4-f51c-4a62-fe0d-6734ecbdf3e4"
   },
   "outputs": [],
   "source": [
    "e.last_extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "8Dn1YIrgJPPV",
    "outputId": "d9f7b40b-334d-41fb-80f1-3a16ddb1a8cf"
   },
   "outputs": [],
   "source": [
    "e.step(e.last_admissible_commands.index('go east'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hW_03R41bt_G"
   },
   "source": [
    "So, the observation from `TextWorldPreproc` is a dict with two keys: \n",
    "* `admissible_commands`: list of commands available in the current state. Index in this array is the action to be pased to `step()` function\n",
    "* `obs`: list of arrays with sequences, by default `description` and `inventory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T3jMq6NeeHM8"
   },
   "source": [
    "## Preprocessor (practice)\n",
    "\n",
    "The next steps in our pipeline are embeddings and encoders, you should implement using template code below. Class `Encoder` is simple wrapper around LSTM layer, and already provided.\n",
    "\n",
    "In **tests** section, you find several use-cases of this class to check your functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9OWcgSQwhlJ"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iA0u5YeCwdgr"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes input sequences (after embeddings) and returns the hidden state from LSTM\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_size: int, out_size: int):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.net = nn.LSTM(input_size=emb_size, hidden_size=out_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.net.flatten_parameters()\n",
    "        _, hid_cell = self.net(x)\n",
    "        # Warn: if bidir=True or several layers, sequeeze has to be changed!\n",
    "        return hid_cell[0].squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GIkU9onpKtYo"
   },
   "source": [
    "Function `order_batch` is used to sort the batch of input sequences by descending of sequence length. This is a PyTorch requirement, which itself is imposed by low-level cudnn implementation.\n",
    "\n",
    "`order_batch` sorts the input batch, returns it and in addition returns the indices of sequences to return the batch to the original order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "muixwId92Klh"
   },
   "outputs": [],
   "source": [
    "def order_batch(batch):\n",
    "    \"\"\"\n",
    "    Order batch of sequences by sequence len and keep inverse\n",
    "    :param batch: list of lists of items\n",
    "    :return: Tuple[sorted batch, numpy array to return the order]\n",
    "    \"\"\"\n",
    "    lens = list(map(len, batch))\n",
    "    ord_idx = np.flip(np.argsort(lens, kind='stable'))\n",
    "    ord_inv = np.argsort(ord_idx, kind='stable')\n",
    "    ord_batch = [batch[idx] for idx in ord_idx]\n",
    "    return ord_batch, ord_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqtHtoYkMw5z"
   },
   "source": [
    "Now, this is the class, you need to implement. Function `_apply_encoder` is the core of class functionality and should apply one `Encoder` instance to the batch of sequences. You should use function [`rnn_utils.pack_sequence`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_sequence) to obtain instance of [`PackedSequence` class](https://pytorch.org/docs/stable/nn.html#packedsequence). \n",
    "\n",
    "This might be tricky if you haven't played with PyTorch RNNs before, so, you probably should first try to apply simple RNN to small toy tensors to understand they idea. Those links are also could be useful:\n",
    "* https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
    "* https://discuss.pytorch.org/t/simple-working-example-how-to-use-packing-for-variable-length-sequence-inputs-for-rnn/2120\n",
    "* https://www.kdnuggets.com/2018/06/taming-lstms-variable-sized-mini-batches-pytorch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XqYjp6G0xAG4"
   },
   "outputs": [],
   "source": [
    "class Preprocessor(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes batch of several input sequences and outputs their summary from one or many encoders\n",
    "    \"\"\"\n",
    "    def __init__(self, dict_size: int, emb_size: int, num_sequences: int, enc_output_size: int):\n",
    "        \"\"\"\n",
    "        :param dict_size: amount of words is our vocabulary\n",
    "        :param emb_size: dimensionality of embeddings\n",
    "        :param num_sequences: count of sequences\n",
    "        :param enc_output_size: output from single encoder\n",
    "        \"\"\"\n",
    "        super(Preprocessor, self).__init__()\n",
    "\n",
    "        # TODO: construct the class\n",
    "        # 1. create nn.Embedding layer\n",
    "        # 2. build a list of Encoder instances for input sequences\n",
    "        # 3. register them in the module using self.add_module() call\n",
    "        # 4. create separate Encoder instance for command encoding\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def encode_sequences(self, batches):\n",
    "        \"\"\"\n",
    "        Encode batches of sequences. Input is a list of observations, each \n",
    "        observation is a list of sequences. Corresponding encoders should\n",
    "        be applied to corresponding sequence.\n",
    "        \n",
    "        More concretely, if we have two sequences: 'description' and 'inventory',\n",
    "        input could be:\n",
    "        \n",
    "        [\n",
    "          [\n",
    "            [tokens of \"you are standing on a cliff\"],\n",
    "            [tokens of \"you have an axe\"]\n",
    "\n",
    "          ],\n",
    "          [\n",
    "            [tokens of \"you're in cosy chair\"],\n",
    "            [tokens of \"you're holding cup of tea\"]\n",
    "          ]\n",
    "        ]\n",
    "        \n",
    "        The output from every encoder should be concatenated, keeping batches \n",
    "        over the first dimension.\n",
    "        \n",
    "        Implemented properly, should be just call to _apply_encoder in a loop \n",
    "        with torch.cat() call\n",
    "        \n",
    "        :param batches: list of tuples with variable-length sequences of word ids\n",
    "        :return: tensor with concatenated encoder outputs for every batch sample\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def encode_commands(self, batch):\n",
    "        \"\"\"\n",
    "        Apply encoder to list of commands sequence.\n",
    "        \n",
    "        Should be just call to _apply_encoder()\n",
    "        \n",
    "        :param batch: list of lists of idx\n",
    "        :return: tensor with encoded commands in original order\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def _apply_encoder(self, batch, encoder):\n",
    "        \"\"\"\n",
    "        A bit tricky internal method which applies encoder to the batch (which\n",
    "        is a list of variable-length sequences).\n",
    "        \n",
    "        You should use order_batch() function above (to properly sort the sequence),\n",
    "        and rnn_utils.pack_sequence and rnn_utils.PackedSequence. Alternatively,\n",
    "        you can use padding, but this will make the implementation less memory-efficient.\n",
    "        \n",
    "        And don't forget about embeddings!\n",
    "        \n",
    "        Output is a tensor of size (len(batch), embeddings, encoder_out_dim)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VtwuS0jB23xC"
   },
   "source": [
    "### Tests\n",
    "\n",
    "Several test cases for the code you've just written. Might be helpful to understand the inputs and outputs of the methods to be implemented, so, don't hesitate to study test cases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "GDzgwXfg3G0i",
    "outputId": "d49a8c5c-bb16-45f9-806e-578944ec2903"
   },
   "outputs": [],
   "source": [
    "# construct our preprocessor, vocab has 10 tokens, embeddings of size 5, two input sequences, Encoder should return vectors of size 7\n",
    "prep = Preprocessor(dict_size=10, emb_size=5, num_sequences=2, enc_output_size=7)\n",
    "prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "EYPH8nH83ftL",
    "outputId": "76f8d178-d87d-441a-e062-9e718e791ad0"
   },
   "outputs": [],
   "source": [
    "# apply encoder to two sequences, should return 2x7 tensor\n",
    "enc = Encoder(emb_size=5, out_size=7)\n",
    "t = prep._apply_encoder([\n",
    "    [1, 2, 3], [2],\n",
    "], enc)\n",
    "print(t)\n",
    "assert t.size() == (2, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "8eJ2wwHH4miR",
    "outputId": "c66c6bcd-bb39-4a66-db19-f66c516a1865"
   },
   "outputs": [],
   "source": [
    "# same batch reversed should have reversed tensor\n",
    "tt = prep._apply_encoder([\n",
    "    [2],\n",
    "    [1, 2, 3], \n",
    "], enc)\n",
    "print(tt)\n",
    "tn = t.detach().numpy()\n",
    "ttn = tt.detach().numpy()\n",
    "np.testing.assert_allclose(tn[0], ttn[1])\n",
    "np.testing.assert_allclose(tn[1], ttn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "quD35tPU5aGl",
    "outputId": "43af221c-55a9-4d23-8d3b-216ca9b6e0ca"
   },
   "outputs": [],
   "source": [
    "# the same semantics as just encoder application, but encoder is different, so, numbers shouldn't match\n",
    "ttt = prep.encode_commands([[1, 2, 3], [2]])\n",
    "print(ttt)\n",
    "assert t.size() == (2, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "mk9_2CiD7jSI",
    "outputId": "407eb847-6225-4e09-9c8e-2cb44325b1c5"
   },
   "outputs": [],
   "source": [
    "# encode sequences, now we have list of batches for every sequence\n",
    "t = prep.encode_sequences([\n",
    "    # obs1\n",
    "    [[1,2,3], [2]],\n",
    "    # obs2\n",
    "    [[1,2,3], [2,2,2]],\n",
    "])\n",
    "print(t)\n",
    "\n",
    "# the output shape should be [batch_size=2, encoder_size*num_sequences=7*2=14]\n",
    "assert t.size() == (2, 14)\n",
    "# another subtle property is that both observation have the same first sequence, \n",
    "# first 7 numbers in both rows should be the same\n",
    "torch.testing.assert_allclose(t[0][:7], t[1][:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "K8FRLCYWQOjb",
    "outputId": "19c12481-07da-491f-f522-3aae654ebb9f"
   },
   "outputs": [],
   "source": [
    "# try inversed sequences\n",
    "tt = prep.encode_sequences([\n",
    "    # obs1\n",
    "    [[1,2,3], [2,2,2]],\n",
    "    # obs2\n",
    "    [[1,2,3], [2]],\n",
    "\n",
    "])\n",
    "print(tt)\n",
    "\n",
    "# the output shape should be [batch_size=2, encoder_size*num_sequences=7*2=14]\n",
    "assert tt.size() == (2, 14)\n",
    "torch.testing.assert_allclose(tt[0][:7], tt[1][:7])\n",
    "torch.testing.assert_allclose(t[0], tt[1])\n",
    "torch.testing.assert_allclose(t[1], tt[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eyqqc-mhkxuL"
   },
   "source": [
    "## Preprocessor (solution)\n",
    "\n",
    "**SPOILER Alert!** If you are not completely lost in previous excercise, it would be better to keep trying (or ask TAs for hints). Below is the complete solution, which might ruin some fun from the excercise :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYnQOJQTlGwk"
   },
   "outputs": [],
   "source": [
    "class Preprocessor(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes batch of several input sequences and outputs their summary from one or many encoders\n",
    "    \"\"\"\n",
    "    def __init__(self, dict_size: int, emb_size: int, num_sequences: int, enc_output_size: int):\n",
    "        \"\"\"\n",
    "        :param dict_size: amount of words is our vocabulary\n",
    "        :param emb_size: dimensionality of embeddings\n",
    "        :param num_sequences: count of sequences\n",
    "        :param enc_output_size: output from single encoder\n",
    "        \"\"\"\n",
    "        super(Preprocessor, self).__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(num_embeddings=dict_size, embedding_dim=emb_size)\n",
    "        self.encoders = []\n",
    "        for idx in range(num_sequences):\n",
    "            enc = Encoder(emb_size, enc_output_size)\n",
    "            self.encoders.append(enc)\n",
    "            self.add_module(f\"enc_{idx}\", enc)\n",
    "        self.enc_commands = Encoder(emb_size, enc_output_size)\n",
    "\n",
    "    def encode_sequences(self, batches):\n",
    "        \"\"\"\n",
    "        Forward pass of Preprocessor\n",
    "        :param batches: list of tuples with variable-length sequences of word ids\n",
    "        :return: tensor with concatenated encoder outputs for every batch sample\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for enc, enc_batch in zip(self.encoders, zip(*batches)):\n",
    "            data.append(self._apply_encoder(enc_batch, enc))\n",
    "        res_t = torch.cat(data, dim=1)\n",
    "        return res_t\n",
    "\n",
    "    def _apply_encoder(self, batch, encoder):\n",
    "        ord_batch, inv_batch = order_batch(batch)\n",
    "        dev = self.emb.weight.device\n",
    "        ord_batch_t = [torch.tensor(sample).to(dev) for sample in ord_batch]\n",
    "        batch_seq = rnn_utils.pack_sequence(ord_batch_t)\n",
    "        emb_seq_t = rnn_utils.PackedSequence(data=self.emb(batch_seq.data), batch_sizes=batch_seq.batch_sizes)\n",
    "        res = encoder(emb_seq_t)\n",
    "        res = res[inv_batch]\n",
    "        return res\n",
    "\n",
    "    def encode_commands(self, batch):\n",
    "        \"\"\"\n",
    "        Apply encoder to list of commands sequence\n",
    "        :param batch: list of lists of idx\n",
    "        :return: tensor with encoded commands in original order\n",
    "        \"\"\"\n",
    "        return self._apply_encoder(batch, self.enc_commands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qmqRvRvI58qB"
   },
   "source": [
    "## DQN model\n",
    "\n",
    "Final step in our diagram is DQN model, which takes the observation representation produced by encoders + encoded command to be executed and produces Q(s, a). \n",
    "\n",
    "The code is trivial and straghtforward, but of course, you can tweak the model if you want (layers, activations, etc).\n",
    "\n",
    "For convenience, in addition to `forward()` method, we implement `q_values()` which applies vector of observations and batch of commands to the model to obtain Q-values for all admissible commands we have in the particular state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFbryTit45f8"
   },
   "outputs": [],
   "source": [
    "class DQNModel(nn.Module):\n",
    "    def __init__(self, obs_size: int, cmd_size: int, hid_size: int = 256):\n",
    "        super(DQNModel, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size + cmd_size, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, cmd):\n",
    "        x = torch.cat((obs, cmd), dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "    def q_values(self, obs_t, commands_t):\n",
    "        \"\"\"\n",
    "        Calculate q-values for observation and tensor of commands\n",
    "        :param obs_t: preprocessed observation, need to be of [1, obs_size] shape\n",
    "        :param commands_t: commands to be evaluated, shape is [N, cmd_size]\n",
    "        :return: list of q-values for commands\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for cmd_t in commands_t:\n",
    "            result.append(self(obs_t, cmd_t.unsqueeze(0))[0].cpu().item())\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5euCik0qBDVG"
   },
   "source": [
    "## Ptan Agent (practice)\n",
    "\n",
    "Now, with all components of our pipeline implemented, we can start working on training process. We'll use Ptan library (https://github.com/Shmuma/ptan) simple PyTorch-based lib for RL. We've covered ptan on the previous tutorial (to be written), so, you might remember that the **agent** is one of the central concepts in ptan.\n",
    "\n",
    "As a reminder, the goal of agent is to convert batch of environment states into batch of actions to be executed. During this conversion, agent could apply exploration policy (by sampling random actions from time to time, for example) and can have an optional state (which is useful for PoMDP cases).\n",
    "\n",
    "In TextWorld we don't need states yet (as with inventory description our env is MDP). Below is the outline of the agent you need to write.\n",
    "\n",
    "Additional function our agent need to implement is epsilon-greedy exploration, which just means taking random action with given probability (denoted by epsilon). If epsilon=0, we always take action with maximum Q value. In case of epsilon=1, actions are always randomly sampled. We will decrease the epsilon during the training to allow agent to explore the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFq7sBRwDFcX"
   },
   "outputs": [],
   "source": [
    "class DQNAgent(ptan.agent.BaseAgent):\n",
    "    def __init__(self, net: DQNModel, preprocessor: Preprocessor, epsilon: float = 0.0, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Construct the agent. We pass the model we've just defined, preprocessor\n",
    "        to convert observations and commands into input vector.\n",
    "        In addition, we allow agent to perform exploration with \n",
    "        given epsilon probability.\n",
    "        \"\"\"\n",
    "        self.net = net\n",
    "        self.preprocessor = preprocessor\n",
    "        self._epsilon = epsilon\n",
    "        self.device = device\n",
    "\n",
    "    @property\n",
    "    def epsilon(self):\n",
    "        return self._epsilon\n",
    "\n",
    "    @epsilon.setter\n",
    "    def epsilon(self, value: float):\n",
    "        if 0.0 <= value <= 1.0:\n",
    "            self._epsilon = value\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, states: List[dict], agent_states: Optional[Any] = None) -> Tuple[List[int], List[Any]]:\n",
    "        \"\"\"\n",
    "        Argument `states` is batch of observations returned by \n",
    "        TextWorldPreproc, which is a dict with two keys: \n",
    "        'obs' and 'admissible_commands'\n",
    "        \"\"\"\n",
    "        if agent_states is None:\n",
    "            agent_states = [None] * len(states)\n",
    "\n",
    "        # for every state in the batch, calculate\n",
    "        actions = []\n",
    "        for state in states:\n",
    "            # for every state, you need to:\n",
    "            # 1. encode observations using preprocessor,\n",
    "            # 2. encode admissible commands\n",
    "            # 3. calculate q-values \n",
    "            # 4. choose the best action to do (index in admissible commands list)\n",
    "            # NB: do no forget about exploration!\n",
    "            raise NotImplementedError\n",
    "        return actions, agent_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uHj48ULXI8ir"
   },
   "source": [
    "### Tests\n",
    "\n",
    "With code below you can check you implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "56Qi-nV6JHPp"
   },
   "outputs": [],
   "source": [
    "# this code cell is also useful to understand how're going to use our classes\n",
    "env = make_env()\n",
    "env = TextWorldPreproc(env)\n",
    "\n",
    "ENC_SIZE = 5\n",
    "prep = Preprocessor(dict_size=env.observation_space.vocab_size, emb_size=20, \n",
    "                    num_sequences=env.num_fields, enc_output_size=ENC_SIZE)\n",
    "\n",
    "net = DQNModel(obs_size=env.num_fields*ENC_SIZE, cmd_size=ENC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04bBSp7ALBWd"
   },
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "\n",
    "# check the epsilon first - make the net to return zeroes all the time\n",
    "net.q_values = lambda obs, cmd: [0.0 for c in cmd]\n",
    "\n",
    "# no exploration, should be zeros\n",
    "agent = DQNAgent(net, prep, epsilon=0.0)\n",
    "acts, _ = agent([s for _ in range(10)])\n",
    "assert acts == [0]*10\n",
    "\n",
    "# exploration all the time, should not be only zeros\n",
    "agent = DQNAgent(net, prep, epsilon=1.0)\n",
    "acts, _ = agent([s for _ in range(10)])\n",
    "assert acts != [0]*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ir_CH5t9MNly",
    "outputId": "00c7ba4b-c37c-44a5-f5e3-8675d1045637"
   },
   "outputs": [],
   "source": [
    "acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zThuJ03sDIOE"
   },
   "source": [
    "## Ptan Agent (solution)\n",
    "\n",
    "**Spoiler alert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ACM5Y1ajDM8m"
   },
   "outputs": [],
   "source": [
    "class DQNAgent(ptan.agent.BaseAgent):\n",
    "    def __init__(self, net: DQNModel, preprocessor: Preprocessor, epsilon: float = 0.0, device=\"cpu\"):\n",
    "        self.net = net\n",
    "        self.preprocessor = preprocessor\n",
    "        self._epsilon = epsilon\n",
    "        self.device = device\n",
    "\n",
    "    @property\n",
    "    def epsilon(self):\n",
    "        return self._epsilon\n",
    "\n",
    "    @epsilon.setter\n",
    "    def epsilon(self, value: float):\n",
    "        if 0.0 <= value <= 1.0:\n",
    "            self._epsilon = value\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, states, agent_states=None):\n",
    "        if agent_states is None:\n",
    "            agent_states = [None] * len(states)\n",
    "\n",
    "        # for every state in the batch, calculate\n",
    "        actions = []\n",
    "        for state in states:\n",
    "            commands = state['admissible_commands']\n",
    "            if random.random() <= self.epsilon:\n",
    "                actions.append(random.randrange(len(commands)))\n",
    "            else:\n",
    "                obs_t = self.preprocessor.encode_sequences([state['obs']]).to(self.device)\n",
    "                commands_t = self.preprocessor.encode_commands(commands).to(self.device)\n",
    "                q_vals = self.net.q_values(obs_t, commands_t)\n",
    "                actions.append(np.argmax(q_vals))\n",
    "        return actions, agent_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmZVNWs6Ucdt"
   },
   "source": [
    "## Loss calculation (practice)\n",
    "\n",
    "Agent class allows us to start gathering training samples from environment. Experience buffering and sampling are already implemented in ptan, so, we'll just use existing classes. The only piece we're missing is **loss calculation**.\n",
    "\n",
    "In DQN we're using MSE loss between net prediction and estimation given by Bellman equation. In our baseline we'll use the simplest 1-step Bellman: \n",
    "\n",
    "$Q_{s,a} = r_{s,a} + \\gamma \\cdot max_{a'}Q_{s',a'}$\n",
    "\n",
    "Below one function is given to you: `unpack_batch`, which takes list of environment steps and convert them into the form, suitable for MSE calculation. Your task is to implement this MSE, which, in fact, just 5 lines of code.\n",
    "\n",
    "**NB** Function `unpack_batch` is not very efficient, as it calculates encoded sequences and commands for every state in the batch. You can make it much more efficient, by combining encoders applications. This is left as an excercise :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Umh9TwX7YIsZ"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def unpack_batch(batch: List[ptan.experience.Experience], preprocessor: Preprocessor,\n",
    "                 net: DQNModel, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Convert batch to data needed for Bellman step\n",
    "    :param batch: list of ptan.Experience objects\n",
    "    :param preprocessor: emb.Preprocessor instance\n",
    "    :param net: network to be used for next state approximation\n",
    "    :param device: torch device\n",
    "    :return: tuple (list of observations, list of taken commands, list of rewards, list of best Qs for the next state)\n",
    "    \"\"\"\n",
    "    # calculate Qs for next states\n",
    "    observations, taken_commands, rewards, best_q = [], [], [], []\n",
    "    for exp in batch:\n",
    "        observations.append(exp.state['obs'])\n",
    "        taken_commands.append(exp.state['admissible_commands'][exp.action])\n",
    "        rewards.append(exp.reward)\n",
    "\n",
    "        # calculate best Q value for the next state\n",
    "        if exp.last_state is None:\n",
    "            # final state in the episode, Q=0\n",
    "            best_q.append(0.0)\n",
    "        else:\n",
    "            obs_t = preprocessor.encode_sequences([exp.last_state['obs']]).to(device)\n",
    "            commands_t = preprocessor.encode_commands(exp.last_state['admissible_commands']).to(device)\n",
    "            q_vals = net.q_values(obs_t, commands_t)\n",
    "            best_q.append(max(q_vals))\n",
    "    return observations, taken_commands, rewards, best_q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0INdyoknYMdO"
   },
   "outputs": [],
   "source": [
    "def calc_loss_dqn(batch, preprocessor, tgt_preprocessor, net, tgt_net, gamma, device=\"cpu\"):\n",
    "    observations, taken_commands, rewards, next_best_qs = unpack_batch(batch, tgt_preprocessor, tgt_net, device)\n",
    "\n",
    "    # Here you need:\n",
    "    # 1. encode observations and commands,\n",
    "    # 2. convert them into q-values predicted by network\n",
    "    # 3. calculate the Bellman approximation using rewards and next_best_qs\n",
    "    # 4. calc MSE between predicted Qs and approximation\n",
    "    # Should be fairly straightforward, it took me 6 lines of code\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M0TiaajAZkUx"
   },
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DZz4ojusZnGU"
   },
   "outputs": [],
   "source": [
    "# this code cell is also useful to understand how we are going to use our classes\n",
    "env = make_env()\n",
    "env = TextWorldPreproc(env)\n",
    "\n",
    "ENC_SIZE = 5\n",
    "prep = Preprocessor(dict_size=env.observation_space.vocab_size, emb_size=20, \n",
    "                    num_sequences=env.num_fields, enc_output_size=ENC_SIZE)\n",
    "\n",
    "net = DQNModel(obs_size=env.num_fields*ENC_SIZE, cmd_size=ENC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "rGjNl0cq9y5R",
    "outputId": "a31f8693-6298-4b21-b5c1-dcbb8dd08800"
   },
   "outputs": [],
   "source": [
    "# do one step in the right direction (that's good to know where to go, right?)\n",
    "s = env.reset()\n",
    "orig_info = env.last_extra_info\n",
    "action = env.last_admissible_commands.index('unlock gate with keycard')\n",
    "ss, r, is_done, _ = env.step(action)\n",
    "info = env.last_extra_info\n",
    "exp = ptan.experience.ExperienceFirstLast(state=s, action=action, reward=float(r), last_state=ss)\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_mrF4-l_5Sh"
   },
   "outputs": [],
   "source": [
    "l = calc_loss_dqn([exp, exp], prep, prep, net, net, 0.9)\n",
    "assert isinstance(l, torch.Tensor) and l.size() == ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XwNvArlkZoWY"
   },
   "source": [
    "## Loss calculation (solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cQF5tBouZqH0"
   },
   "outputs": [],
   "source": [
    "def calc_loss_dqn(batch, preprocessor, tgt_preprocessor, net, tgt_net, gamma, device=\"cpu\"):\n",
    "    observations, taken_commands, rewards, next_best_qs = unpack_batch(batch, tgt_preprocessor, tgt_net, device)\n",
    "\n",
    "    obs_t = preprocessor.encode_sequences(observations).to(device)\n",
    "    cmds_t = preprocessor.encode_commands(taken_commands).to(device)\n",
    "    q_values_t = net(obs_t, cmds_t)\n",
    "    tgt_q_t = torch.tensor(rewards) + gamma * torch.tensor(next_best_qs)\n",
    "    tgt_q_t = tgt_q_t.to(device)\n",
    "    return F.mse_loss(q_values_t.squeeze(-1), tgt_q_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4MlZBeJZLWm"
   },
   "source": [
    "## Fast batch unpack (solution)\n",
    "\n",
    "About 2x faster than naive version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvDDvGxvZKfw"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def unpack_batch(batch: List[ptan.experience.Experience], preprocessor: Preprocessor,\n",
    "                      net: DQNModel, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Convert batch to data needed for Bellman step\n",
    "    :param batch: list of ptan.Experience objects\n",
    "    :param preprocessor: emb.Preprocessor instance\n",
    "    :param net: network to be used for next state approximation\n",
    "    :param device: torch device\n",
    "    :return: tuple (list of observations, list of taken commands, list of rewards, list of best Qs for the next state)\n",
    "    \"\"\"\n",
    "    # calculate Qs for next states\n",
    "    observations, taken_commands, rewards, best_q = [], [], [], []\n",
    "    last_obs, last_commands, last_offsets = [], [], []\n",
    "    for exp in batch:\n",
    "        observations.append(exp.state['obs'])\n",
    "        taken_commands.append(exp.state['admissible_commands'][exp.action])\n",
    "        rewards.append(exp.reward)\n",
    "\n",
    "        # calculate best Q value for the next state\n",
    "        if exp.last_state is None:\n",
    "            # final state in the episode, Q=0\n",
    "            last_offsets.append(len(last_commands))\n",
    "        else:\n",
    "            last_obs.append(exp.last_state['obs'])\n",
    "            last_commands.extend(exp.last_state['admissible_commands'])\n",
    "            last_offsets.append(len(last_commands))\n",
    "\n",
    "    obs_t = preprocessor.encode_sequences(last_obs).to(device)\n",
    "    commands_t = preprocessor.encode_commands(last_commands).to(device)\n",
    "\n",
    "    prev_ofs = 0\n",
    "    obs_ofs = 0\n",
    "    for ofs in last_offsets:\n",
    "        if prev_ofs == ofs:\n",
    "            best_q.append(0.0)\n",
    "        else:\n",
    "            q_vals = net.q_values(obs_t[obs_ofs:obs_ofs+1], commands_t[prev_ofs:ofs])\n",
    "            best_q.append(max(q_vals))\n",
    "            obs_ofs += 1\n",
    "        prev_ofs = ofs\n",
    "    return observations, taken_commands, rewards, best_q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JUhqwAvwEX5j"
   },
   "source": [
    "## Training loop\n",
    "\n",
    "Ok, we're now fully prepared for training our agent, whee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIFbvnn1TKK0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "import torch.optim as optim\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "ENC_SIZE = 20\n",
    "EMB_SIZE = 20\n",
    "REPLAY_SIZE = 10000\n",
    "REPLAY_INITIAL = 100\n",
    "GAMMA = 0.9\n",
    "LEARNING_RATE = 5e-5\n",
    "SYNC_NETS = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "INITIAL_EPSILON = 1.0\n",
    "FINAL_EPSILON = 0.2\n",
    "STEPS_EPSILON = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g9HVFupZA6QF"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")  # you should use GPU, CPU is way too slow\n",
    "env = make_env()\n",
    "env = TextWorldPreproc(env)\n",
    "\n",
    "prep = Preprocessor(dict_size=env.observation_space.vocab_size, emb_size=EMB_SIZE, \n",
    "                    num_sequences=env.num_fields, enc_output_size=ENC_SIZE)\n",
    "prep = prep.to(device)\n",
    "# we'll use target network to disentangle target predictions in Bellman\n",
    "tgt_prep = ptan.agent.TargetNet(prep)\n",
    "\n",
    "net = DQNModel(obs_size=env.num_fields*ENC_SIZE, cmd_size=ENC_SIZE)\n",
    "net = net.to(device)\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "agent = DQNAgent(net, prep, epsilon=INITIAL_EPSILON, device=device)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, steps_count=1)\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_SIZE)\n",
    "\n",
    "optimizer = optim.RMSprop(itertools.chain(net.parameters(), prep.parameters()),\n",
    "                          lr=LEARNING_RATE, eps=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "mbmmqabCTV6s",
    "outputId": "efe946dc-d15e-4173-fab4-467d489d15d6"
   },
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "episodes_done = 0\n",
    "losses = []\n",
    "rewards = []\n",
    "prev_steps = 0\n",
    "start_ts = prev_ts = time.time()\n",
    "\n",
    "for _ in range(10000):\n",
    "    steps_done += 1\n",
    "    buffer.populate(1)\n",
    "    rewards_steps = exp_source.pop_rewards_steps()\n",
    "    if rewards_steps:\n",
    "        speed = (steps_done - prev_steps) / (time.time() - prev_ts)\n",
    "        prev_steps = steps_done\n",
    "        prev_ts = time.time()\n",
    "        for rw, steps in rewards_steps:\n",
    "            episodes_done += 1\n",
    "            print(\"%d: Done %d episodes: reward = %.2f, steps = %d, speed = %.2f steps/sec, epsilon = %.2f\" % (\n",
    "                steps_done, episodes_done, rw, steps, speed, agent.epsilon))\n",
    "            rewards.append(rw)\n",
    "        if rewards and np.mean(rewards[-10:]) == 6.0:\n",
    "            print(\"Environment has been solved in %s, congrats!\" % datetime.timedelta(seconds=time.time() - start_ts))\n",
    "            break\n",
    "    if len(buffer) < REPLAY_INITIAL:\n",
    "        continue\n",
    "        \n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    optimizer.zero_grad()\n",
    "    loss_t = calc_loss_dqn(batch, prep, tgt_prep.target_model,\n",
    "                           net, tgt_net.target_model, GAMMA, device=device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss_t.item())\n",
    "    \n",
    "    if steps_done % SYNC_NETS == 0:\n",
    "        tgt_prep.sync()\n",
    "        tgt_net.sync()\n",
    "        print(\"%d: sync nets\" % steps_done)\n",
    "        \n",
    "    agent.epsilon = max(FINAL_EPSILON, INITIAL_EPSILON - steps_done / STEPS_EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "maL3A_VlUJgh",
    "outputId": "f11ea368-655b-41ea-85a4-abfd169951fc"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "14pHKgfPXNga",
    "outputId": "26d8a395-9024-41f4-95e0-c354ea17e838"
   },
   "outputs": [],
   "source": [
    "plt.plot(rewards);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g3_rGfRhcMFy"
   },
   "source": [
    "## Check the agent in action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bVYJjt_yctdJ"
   },
   "outputs": [],
   "source": [
    "test_env = make_env()\n",
    "test_env = TextWorldPreproc(test_env)\n",
    "test_agent = DQNAgent(net, prep, epsilon=0, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itphK_4tdu_w"
   },
   "outputs": [],
   "source": [
    "total_reward = 0\n",
    "step_idx = 0\n",
    "s = test_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "HTsk2hBFdzj5",
    "outputId": "3c420a19-ca85-46ac-907e-2b545b2233b9"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    step_idx += 1\n",
    "    actions, _ = test_agent([s])\n",
    "    action = actions[0]\n",
    "    action_text = test_env.last_admissible_commands[action]\n",
    "    s, r, is_done, _ = test_env.step(action)\n",
    "    total_reward += r\n",
    "    print(\"%d: %s -> reward=%s, total_reward=%s\" % (step_idx, action_text, r, total_reward))\n",
    "    if is_done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mngpKw3WfKrh"
   },
   "source": [
    "# Further steps to explore\n",
    "\n",
    "Lots of!\n",
    "\n",
    "*  We're feeding one command at a time in our DQN, maybe, [Pointer networks (1506.03134)](https://arxiv.org/abs/1506.03134) will be more suitable model for this. (Pointer Networks is a tweak to attention-based model to produce variable-sized outputs)\n",
    "*   We've simplified our life a lot with `intermediate_rewards`, can we solve the game without it? (very likely more advanced exploration will be needed)\n",
    "* Lots of opportunities to check different exploration strategies (curiosity-driven exploration, for example)\n",
    "* Solve one specific game is boring, by tweaking `TextWorldPreproc` we can combine several games together to sample from different games.\n",
    "* What about transfer learning? Can we train on one set of games to solve another games?\n",
    "* We're using `admissible_commands`, which looks like a cheating. Can you train proper command generator?\n",
    "\n",
    "And, finally, could you train agent to solve zork?\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eyqqc-mhkxuL",
    "zThuJ03sDIOE",
    "XwNvArlkZoWY",
    "o4MlZBeJZLWm"
   ],
   "name": "RLSS19-TextWorld.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
