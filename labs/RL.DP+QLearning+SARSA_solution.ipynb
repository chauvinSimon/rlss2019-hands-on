{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pOdksYCAQjQa"
   },
   "source": [
    "##### Run this cell to set your notebook up (only mandatory if rlss2019-docker image is not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LmW_H3Z6QjQh"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/yfletberliac/rlss2019-hands-on.git > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72y9j2sfQjQv"
   },
   "source": [
    "# Reinforcement Learning - Practical Session 1\n",
    "\n",
    "\n",
    "## Review\n",
    "\n",
    "A Markov Decision Process (MDP) is defined as tuple $(S, A, P, r, \\gamma)$ where:\n",
    "* $S$ is the state space\n",
    "* $A$ is the action space \n",
    "* $P$ represents the transition probabilities, $P(s,a,s')$ is the probability of arriving at state $s'$ by taking action $a$ in state $s$\n",
    "* $r$ is the reward function such that $r(s,a,s')$ is the reward obtained by taking action $a$ in state $s$ and arriving at $s'$\n",
    "* $\\gamma$ is the discount factor\n",
    "\n",
    "A deterministic policy $\\pi$ is a mapping from $S$ to $A$: $\\pi(s)$ is the action to be taken at state $s$.\n",
    "\n",
    "The goal of an agent is to find the policy $\\pi$ that maximizes the expected sum of discounted rewards by following $\\pi$. The value of $\\pi$ is defined as\n",
    "\n",
    "$$\n",
    "V_\\pi(s) = E\\left[ \\sum_{t=0}^\\infty \\gamma^t r(S_t, A_t, S_{t+1}) | S_0 = s \\right]\n",
    "$$\n",
    "\n",
    "$V_\\pi(s)$ and the optimal value function, defined as $V^*(s) = \\max_\\pi V_\\pi(s)$, can be shown to satisfy the Bellman equations:\n",
    "\n",
    "$$\n",
    "V_\\pi(s) = \\sum_{s' \\in S}  P(s,\\pi(s),s')[r(s,\\pi(s),s') + \\gamma V_\\pi(s')]\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "V^*(s) = \\max_{a\\in A} \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma V^*(s')]\n",
    "$$\n",
    "\n",
    "It is sometimes better to work with Q functions:\n",
    "\n",
    "$$\n",
    "Q_\\pi(s, a) = \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma  Q^\\pi(s', \\pi(s')]\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "Q^*(s, a) = \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma \\max_{a'} Q^*(s', a')]\n",
    "$$\n",
    "\n",
    "such that $V_\\pi(s) = Q_\\pi(s, \\pi(s))$ and $V^*(s) = \\max_a Q^*(s, a)$.\n",
    "\n",
    "\n",
    "### Using value iteration to compute an optimal policy\n",
    "If the reward function and the transition probabilities are known (and the state and action spaces are not very large), we can use dynamic programming methods to compute $V^*(s)$. Value iteration is one way to do that.\n",
    "\n",
    "\n",
    "#####  Value iteration to compute $V^*(s)$\n",
    "$$\n",
    "T^* Q(s,a) = \\sum_{s'}P(s'|s,a)[ r(s, a, s') + \\gamma \\max_{a'} Q(s', a')]   \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "* For any $Q_0$, let $Q_n = T^* Q_{n-1}$. \n",
    "* We have $\\lim_{n\\to\\infty}Q_n = Q^*$ and $Q^* = T^* Q^*$\n",
    "\n",
    "\n",
    "##### Finding the optimal policy from $V^\\pi(s)$\n",
    "\n",
    "The optimal policy $\\pi^*$ can be computed as\n",
    "\n",
    "$$\n",
    "\\pi^*(s) \\in \\arg\\max_{a\\in A} Q^*(s, a) =  \\arg\\max_{a\\in A} \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma V^*(s')]\n",
    "$$\n",
    "\n",
    "###  Q-Learning and SARSA \n",
    "\n",
    "When the reward function and the transition probabilities are *unknown*, we cannot use dynamic programming to find the optimal value function. Q-Learning and SARSA are stochastic approximation algorithms that allows us to estimate the value function by using only samples from the environment.\n",
    "\n",
    "#####  Q-learning\n",
    "\n",
    "The Q-Learning algorithm allows us to estimate the optimal Q function using only trajectories from the MDP obtained by following some exploration policy. \n",
    "\n",
    "Q-learning with $\\varepsilon$-greedy exploration does the following update at time $t$:\n",
    "\n",
    "1. In state $s_t$, take action $a_t$  such that $a_t$ is random with probability $\\varepsilon$ and $a_t \\in \\arg\\max_a \\hat{Q}_t(s_t,a) $ with probability $1-\\varepsilon$;\n",
    "2. Observe $s_{t+1}$ and reward $r_t$;\n",
    "3. Compute $\\delta_t = r_t + \\gamma \\max_a \\hat{Q}_t(s_{t+1}, a) - \\hat{Q}_t(s_t, a_t)$;\n",
    "4. Update $\\hat{Q}_{t+1}(s, a) = \\hat{Q}_t(s, a) + \\alpha_t(s,a)\\delta_t\\mathbb{1}\\{s=s_t, a=a_t\\}  $\n",
    "\n",
    "\n",
    "##### SARSA\n",
    "\n",
    "SARSA is similar to Q-learning, but it is an *on-policy* algorithm: it follows a (stochastic) policy $\\pi_Q$ and updates its estimate towards the value of this policy. One possible choice is:\n",
    "\n",
    "$$\n",
    "\\pi_Q(a|s) = \\frac{ \\exp(\\tau^{-1}Q(s,a))  }{\\sum_{a'}\\exp(\\tau^{-1}Q(s,a')) }\n",
    "$$\n",
    "where $\\tau$ is a \"temperature\" parameter: when $\\tau$ approaches 0, $\\pi_Q(a|s)$ approaches the greedy (deterministic) policy $a \\in \\arg\\max_{a'}Q(s,a')$.\n",
    "\n",
    "At each time $t$, SARSA keeps an estimate $\\hat{Q}_t$ of the true Q function and uses $\\pi_{\\hat{Q}_t}(a|s)$ to choose the action $a_t$. If $\\tau \\to 0$ with a proper rate as $t \\to \\infty$, $\\hat{Q}_t$ converges to $Q$ and $\\pi_{\\hat{Q}_t}(a|s)$ converges to the optimal policy $\\pi^*$. \n",
    "\n",
    "The SARSA update at time $t$ is done as follows:\n",
    "\n",
    "1. In state $s_t$, take action $a_t \\sim \\pi_{\\hat{Q}_t}(a|s_t)$ ;\n",
    "2. Observe $s_{t+1}$ and reward $r_t$;\n",
    "3. Sample the next action $a_{t+1} \\sim \\pi_{\\hat{Q}_t}(a|s_{t+1})$;\n",
    "4. Compute $\\delta_t = r_t + \\gamma \\hat{Q}_t(s_{t+1}, a_{t+1}) - \\hat{Q}_t(s_t, a_t)$\n",
    "5. Update $\\hat{Q}_{t+1}(s, a) = \\hat{Q}_t(s, a) + \\alpha_t(s,a)\\delta_t\\mathbb{1}\\{s=s_t, a=a_t\\}$\n",
    "\n",
    "## Goals\n",
    "\n",
    "Your goal is to implement Value Iteration, Q-Learning and SARSA for the [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/) environment.\n",
    "\n",
    "* In exercise 1, you will implement the Bellman operators $T^\\pi$ and $T^*$ and verify their properties.\n",
    "* In exercise 2, you will implement value iteration\n",
    "* In exercises 3 and 4, you will implement Q-Learning and SARSA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iz-CDZgBQjQx"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.insert(0, './rlss2019-hands-on/utils')\n",
    "# If using the Docker image, replace by:\n",
    "sys.path.insert(0, '../utils')\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax # for SARSA\n",
    "import matplotlib.pyplot as plt\n",
    "from frozen_lake import FrozenLake\n",
    "from test_env import ToyEnv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vx81eEx-QjQ7"
   },
   "source": [
    "# FrozenLake environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "PnP-CsIUQjQ9",
    "outputId": "a696dcd2-7835-4bad-b8f0-6c887c5bed2e"
   },
   "outputs": [],
   "source": [
    "# Creating an instance of FrozenLake\n",
    "# --- If deterministic=False, transitions are stochastic. Try both cases!\n",
    "# env = FrozenLake(gamma=0.95, deterministic=False, data_path=\"./rlss2019-hands-on/data\") \n",
    "# If using the Docker image, replace by:\n",
    "env = FrozenLake(gamma=0.95, deterministic=False, data_path=\"../data\") \n",
    "# Small environment for debugging\n",
    "# env = ToyEnv1(gamma=0.95)\n",
    "\n",
    "# Useful attributes\n",
    "print(\"Set of states:\", env.states)\n",
    "print(\"Set of actions:\", env.actions)\n",
    "print(\"Number of states: \", env.Ns)\n",
    "print(\"Number of actions: \", env.Na)\n",
    "print(\"P has shape: \", env.P.shape)  # P[s, a, s'] = env.P[s, a, s']\n",
    "print(\"discount factor: \", env.gamma)\n",
    "print(\"\")\n",
    "\n",
    "# Useful methods\n",
    "state = env.reset() # get initial state\n",
    "print(\"initial state: \", state)\n",
    "print(\"reward at (s=1, a=3,s'=2): \", env.reward_func(1,3,2))\n",
    "print(\"\")\n",
    "\n",
    "# A random policy\n",
    "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
    "print(\"random policy = \", policy)\n",
    "\n",
    "# Interacting with the environment\n",
    "print(\"(s, a, s', r):\")\n",
    "for time in range(4):\n",
    "    action = policy[state]\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    print(state, action, next_state, reward)\n",
    "    if done:\n",
    "        break\n",
    "    state = next_state\n",
    "print(\"\")\n",
    "\n",
    "# Visualizing the environment\n",
    "try:\n",
    "    env.render()\n",
    "except:\n",
    "    pass # render not available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbWEUdr9QjRU"
   },
   "source": [
    "# Exercise 1: Bellman operator\n",
    "\n",
    "1. Write a function that takes an environment and a state-action value function $Q$ as input and returns the Bellman optimality operator applied to $Q$, $T^* Q$ and the greedy policy with respect to $Q$.\n",
    "3. Let $Q_1$ and $Q_2$ be state-action value functions. Verify the contraction property:  $\\Vert T^* Q_1 - T^* Q_2\\Vert \\leq \\gamma ||Q_1 - Q_2||$, where $||V|| = \\max_{s,a} |Q(s,a)|$.\n",
    "\n",
    "Recall $$\n",
    "T^* Q(s,a) = \\sum_{s'}P(s'|s,a)[ r(s, a, s') + \\gamma \\max_{a'} Q(s', a')]   \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0uHxg1uaQjRa"
   },
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Solution to 1.\n",
    "# --------------\n",
    "def bellman_operator(Q, env):\n",
    "    TQ = np.zeros((env.Ns, env.Na))\n",
    "    greedy_policy = np.zeros(env.Ns)\n",
    "    for s in env.states:\n",
    "        for a in env.actions:\n",
    "            prob = env.P[s, a, :]\n",
    "            rewards = np.array([float(env.reward_func(s,a, s_)) for s_ in env.states])\n",
    "            TQ[s,a] = np.sum( prob*(rewards + env.gamma*Q.max(axis=1))  )\n",
    "\n",
    "    greedy_policy = np.argmax(TQ, axis = 1)\n",
    "    \n",
    "    return TQ, greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "uSpJ4RAHQjRs",
    "outputId": "7fdfae38-5609-4709-8946-a17b718ecab0"
   },
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Solution to 2.\n",
    "# --------------\n",
    "n_simulations = 200\n",
    "\n",
    "print(\"Contraction of Bellman operator: \")\n",
    "contractions = []\n",
    "for ii in range(n_simulations):\n",
    "    Q1 = np.random.randn(env.Ns, env.Na)\n",
    "    Q2 = np.random.randn(env.Ns, env.Na)\n",
    "\n",
    "    # Contraction of Bellman operator\n",
    "    contractions.append(np.abs(bellman_operator(Q1, env)[0] - bellman_operator(Q2, env)[0]).max() / np.abs(Q1-Q2).max())\n",
    "    \n",
    "plt.plot(contractions)\n",
    "plt.axhline(env.gamma, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aSx1e54QQjR5"
   },
   "source": [
    "# Exercise 2: Value iteration\n",
    "\n",
    "1. (Optimal Value function) Write a function that takes as input an initial state-action value function `Q0` and an environment `env` and returns a vector `Q` such that $||T^* Q -  Q ||_\\infty \\leq \\varepsilon $ and the greedy policy with respect to $Q$.\n",
    "2. Test the convergence of the function you implemented.\n",
    "3. Display $Q$ and $V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TuiSg2oIQjSB"
   },
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Solution to 1.\n",
    "# --------------\n",
    "def value_iteration(Q0, env, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Finding the optimal value function. To be done!\n",
    "    \"\"\"\n",
    "    Q = Q0\n",
    "    while True:\n",
    "        TQ, greedy_policy = bellman_operator(Q, env)\n",
    "\n",
    "        err = np.abs(TQ-Q).max() \n",
    "        if err < epsilon:\n",
    "            return TQ, greedy_policy\n",
    "\n",
    "        Q = TQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JoOLmKHbQjSb",
    "outputId": "f22590dd-c7e4-44cb-9ac9-c2bfdfea441b"
   },
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Solution to 2.\n",
    "# --------------\n",
    "epsilon = 1e-6\n",
    "Q0 = np.zeros((env.Ns, env.Na))\n",
    "\n",
    "Q, greedy_policy = value_iteration(Q0, env, epsilon)\n",
    "err = np.abs(Q - bellman_operator(Q, env)[0]).max()\n",
    "print(\"norm of T(Q) - Q = \", err)\n",
    "assert err <= epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "7gkCK4YCj3z1",
    "outputId": "86cb0a3a-6dc7-4173-9237-cd23348d7efc"
   },
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Solution to 3.\n",
    "# --------------\n",
    "V = Q.max(axis=1)\n",
    "plt.imshow(V.reshape((4,4)))\n",
    "plt.title(\"V(s)\")\n",
    "plt.show()\n",
    "\n",
    "Q_show = np.zeros((12,12))\n",
    "for x in range(4):\n",
    "  for y in range(4):\n",
    "    Q_show[3*x][3*y+1] = Q[x*4+y][3] # top\n",
    "    Q_show[3*x+1][3*y] = Q[x*4+y][0] # left\n",
    "    Q_show[3*x+2][3*y+1] = Q[x*4+y][1] # bottom\n",
    "    Q_show[3*x+1][3*y+2] = Q[x*4+y][2] # right\n",
    "plt.imshow(Q_show)\n",
    "plt.title(\"Q(s, a)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "viEDuDsMQjSq"
   },
   "source": [
    "# Exercise 3: Q-Learning\n",
    "\n",
    "Implement Q-learning and test its convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gq_LD5WZQjSs"
   },
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Q-Learning implementation\n",
    "# ------------------------------\n",
    "\n",
    "class QLearning:\n",
    "    \"\"\"\n",
    "    Implements Q-learning algorithm with epsilon-greedy exploration\n",
    "\n",
    "    If learning_rate is None; alpha(x,a) = 1/max(1, N(s,a))**alpha\n",
    "    \"\"\"\n",
    "    def __init__(self, env, gamma, alpha=0.6, learning_rate=None, min_learning_rate=0.01, epsilon=1.0, epsilon_decay=0.9995,\n",
    "                 epsilon_min=0.25, seed=42):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.Q = np.zeros((env.Ns, env.Na))\n",
    "        self.Nsa = np.zeros((env.Ns, env.Na))\n",
    "        self.state = env.reset()\n",
    "        self.RS = np.random.RandomState(seed)\n",
    "\n",
    "    def get_delta(self, r, x, a, y, done):\n",
    "        \"\"\"\n",
    "        :param r: reward\n",
    "        :param x: current state\n",
    "        :param a: current action\n",
    "        :param y: next state\n",
    "        :param done:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        max_q_y_a = self.Q[y, :].max()\n",
    "        q_x_a = self.Q[x, a]\n",
    "\n",
    "        return r + self.gamma*max_q_y_a - q_x_a\n",
    "\n",
    "    def get_learning_rate(self, s, a):\n",
    "        if self.learning_rate is None:\n",
    "            return max(1.0/max(1.0, self.Nsa[s, a])**self.alpha, self.min_learning_rate)\n",
    "        else:\n",
    "            return max(self.learning_rate, self.min_learning_rate)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if self.RS.uniform(0, 1) < self.epsilon:\n",
    "            # explore\n",
    "            return np.random.choice(self.env.actions)\n",
    "        else:\n",
    "            # exploit\n",
    "            a = self.Q[state, :].argmax()\n",
    "            return a\n",
    "\n",
    "    def step(self):\n",
    "        # Current state\n",
    "        x = self.env.state\n",
    "\n",
    "        # Choose action\n",
    "        a = self.get_action(x)\n",
    "\n",
    "        # Learning rate\n",
    "        alpha = self.get_learning_rate(x, a)\n",
    "\n",
    "        # Take step\n",
    "        observation, reward, done, info = self.env.step(a)\n",
    "        y = observation\n",
    "        r = reward\n",
    "        delta = self.get_delta(r, x, a, y, done)\n",
    "\n",
    "        # Update\n",
    "        self.Q[x, a] = self.Q[x, a] + alpha*delta\n",
    "\n",
    "        self.Nsa[x, a] += 1\n",
    "        \n",
    "        if done:\n",
    "            # print(x, observation, reward)\n",
    "            self.epsilon = max(self.epsilon*self.epsilon_decay, self.epsilon_min)\n",
    "            self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LOmzktfHQjS6",
    "outputId": "a1ae3acd-083c-4fa6-ebee-d9c2d21ecaa8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Convergence of Q-Learning\n",
    "# ---------------------------\n",
    "\n",
    "# Number of Q learning iterations\n",
    "n_steps = int(5e5)  \n",
    "#n_steps = 10\n",
    "\n",
    "# Get optimal value function and its greedy policy\n",
    "Q0 = np.zeros((env.Ns, env.Na))\n",
    "Q_opt, pi_opt = value_iteration(Q0, env, epsilon=1e-6)\n",
    "\n",
    "# Create qlearning object\n",
    "qlearning = QLearning(env, gamma=env.gamma)\n",
    "\n",
    "# Iterate\n",
    "tt = 0\n",
    "Q_est = np.zeros((n_steps, env.Ns, env.Na))\n",
    "while tt < n_steps:\n",
    "    qlearning.step()\n",
    "    # Store estimate of Q*\n",
    "    Q_est[tt, :, :] = qlearning.Q\n",
    "    tt +=1\n",
    "    \n",
    "\n",
    "# Compute greedy policy (with estimated Q)\n",
    "greedy_policy = np.argmax(qlearning.Q, axis=1)\n",
    "\n",
    "# Plot\n",
    "diff = np.abs(Q_est - Q_opt).mean(axis=(1,2))\n",
    "plt.plot(diff)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('Error')\n",
    "plt.title(\"Q-learning convergence\")\n",
    "\n",
    "print(env.render())\n",
    "print(\"optimal policy: \", pi_opt)\n",
    "print(\"est policy:\", greedy_policy)\n",
    "\n",
    "for state in env.states:\n",
    "    print(state)\n",
    "    print(\"true: \", Q_opt[state, :])\n",
    "    print(\"est: \", Q_est[-1,state, :])\n",
    "    print(\"----------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkD9ZMc_QjTH"
   },
   "source": [
    "# Exercise 4: SARSA\n",
    "\n",
    "Implement SARSA and test its convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "duv55SsBQjTJ"
   },
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# SARSA implementation\n",
    "# ------------------------------\n",
    "\n",
    "class Sarsa:\n",
    "    \"\"\"\n",
    "    Implements SARSA algorithm.\n",
    "\n",
    "    If learning_rate is None; alpha(x,a) = 1/max(1, N(s,a))**alpha\n",
    "    \"\"\"\n",
    "    def __init__(self, env, gamma, alpha=0.6, learning_rate=None, min_learning_rate=0.01, tau=1.0, tau_decay=0.9995,\n",
    "                 tau_min=0.25, seed=42):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.tau = tau\n",
    "        self.tau_decay = tau_decay\n",
    "        self.tau_min = tau_min\n",
    "        self.Q = np.zeros((env.Ns, env.Na))\n",
    "        self.Nsa = np.zeros((env.Ns, env.Na))\n",
    "        self.state = env.reset()\n",
    "        self.RS = np.random.RandomState(seed)\n",
    "\n",
    "    def get_delta(self, r, x, a, y, next_a, done):\n",
    "        \"\"\"\n",
    "        :param r: reward\n",
    "        :param x: current state\n",
    "        :param a: current action\n",
    "        :param y: next state\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        q_y_a = self.Q[y, next_a]\n",
    "        q_x_a = self.Q[x, a]\n",
    "\n",
    "        return r + self.gamma*q_y_a - q_x_a\n",
    "\n",
    "    def get_learning_rate(self, s, a):\n",
    "        if self.learning_rate is None:\n",
    "            return max(1.0/max(1.0, self.Nsa[s, a])**self.alpha, self.min_learning_rate)\n",
    "        else:\n",
    "            return max(self.learning_rate, self.min_learning_rate)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        q = self.Q[state, :]\n",
    "        prob = softmax(q/self.tau)\n",
    "        a = np.random.choice(self.env.actions, p=prob)\n",
    "        return a\n",
    "\n",
    "    def step(self):\n",
    "        # Current state\n",
    "        x = self.env.state\n",
    "\n",
    "        # Choose action\n",
    "        a = self.get_action(x)\n",
    "\n",
    "        # Learning rate\n",
    "        alpha = self.get_learning_rate(x, a)\n",
    "\n",
    "        # Take step\n",
    "        observation, reward, done, info = self.env.step(a)\n",
    "        y = observation\n",
    "        r = reward\n",
    "        next_a = self.get_action(y)\n",
    "        delta = self.get_delta(r, x, a, y, next_a, done)\n",
    "\n",
    "        # Update\n",
    "        self.Q[x, a] = self.Q[x, a] + alpha*delta\n",
    "\n",
    "        self.Nsa[x, a] += 1\n",
    "\n",
    "        if done:\n",
    "            # print(x, observation, reward)\n",
    "            self.tau = max(self.tau*self.tau_decay, self.tau_min)\n",
    "            self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "o1bZtS5lQjTR",
    "outputId": "1b3109d4-a3c2-45e4-c649-ec3927f5cc80"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Convergence of SARSA\n",
    "# ---------------------------\n",
    "\n",
    "# Create SARSA object\n",
    "sarsa = Sarsa(env, gamma=env.gamma)\n",
    "\n",
    "# Iterate\n",
    "tt = 0\n",
    "Q_est = np.zeros((n_steps, env.Ns, env.Na))\n",
    "while tt < n_steps:\n",
    "    sarsa.step()\n",
    "    # Store estimate of Q*\n",
    "    Q_est[tt, :, :] = sarsa.Q\n",
    "    tt +=1\n",
    "\n",
    "# Compute greedy policy (with estimated Q)\n",
    "greedy_policy = np.argmax(sarsa.Q, axis=1)\n",
    "\n",
    "# Plot\n",
    "diff = np.abs(Q_est - Q_opt).mean(axis=(1,2))\n",
    "plt.plot(diff)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('Error')\n",
    "plt.title(\"SARSA\")\n",
    "\n",
    "\n",
    "print(env.render())\n",
    "print(\"optimal policy: \", pi_opt)\n",
    "print(\"est policy:\", greedy_policy)\n",
    "\n",
    "for state in env.states:\n",
    "    print(state)\n",
    "    print(\"true: \", Q_opt[state, :])\n",
    "    print(\"est: \", Q_est[-1,state, :])\n",
    "    print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6SZTfCUQjTf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "RL.DP+QLearning+SARSA_solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "rlss2019",
   "language": "python",
   "name": "rlss2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
